\onehalfspacing
\begin{resumo}

    A presente tese cujo tema central consiste em melhorar a
    eficiência na utilização de dados, a transparência, e a
    compacidade de redes neuronais, propõe novos avanços em técnicas de
    esparsidade e supervisão fraca, defiando a doutrina de que
    modelos neuronais não são capazes de exibir tais características.

    Para atingir eficiência na utilização de dados, propomos
    aproveitar as capacidades de transferência de aprendizagem de
    redes neuronais pré-treinadas em tarefas sequenciais. Aplicamos
    este método em Pós-Edição Automática (PEA) para demonstrar a sua
    eficácia. Para isso, usamos um modelo neuronal pré-treinado nos
    dois componentes principais do sistema de PEA, explorando várias
    estratégias de compartilhamento de parâmetros. Conseguimos obter
    resultados competitivos, quando comparado com modelos treinados
    em grandes quantidades de dados artificiais, mas utilizando
    apenas um pequeno conjunto de dados.

    Para atingir transparência, propomos substituir a função softmax
    nos mecanismos de atenção de redes neuronais pela função
    $\alpha$-entmax: uma generalização diferenciável da softmax que
    permite que palavras com baixa pontuação recebam precisamente um
    peso de zero. Para além disso, derivamos um método para aprender
    o parâmetro $\alpha$ automaticamente, que controla a forma e a
    dispersão da função $\alpha$-entmax. Em tradução automática,
    mostramos que a nossa abordagem adaptativa melhora a
    interpretabilidade quando comparado à simples aplicação da
    softmax.

    Para atingir modelos compactos, propomos uma nova técnica para
    treinar modelos com variáveis latentes discretas. Treinar este
    tipo de modelos é muitas vezes desafiador devido à necessidade de
    usar estratégias de amostragem, que têm uma variância alta, ou
    por ter de usar relaxamentos para o espaço contínuo. Em
    alternativa, propomos usar mapeamentos esparsos diferenciáveis
    para parametrizar as distribuições discretas. A esparsidade
    permite que se faça uma marginalização eficiente das variáveis
    latentes, em vez da utilização das técnicas mencionadas. Usamos o
    nosso método em três diferentes aplicações e obtemos resultados
    competitivos atingindo, mesmo assim, a praticabilidade de
    aproximações baseadas em amostragem.

\end{resumo}
