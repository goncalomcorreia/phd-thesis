\onehalfspacing
\begin{resumo}

    Esta tese propõe novas técnicas para melhorar a eficiência na
    utilização de dados, a transparência e a compacidade de redes
    neuronais, com aplicações em processamento de linguagem
    natural. Graças aos nossos avanços em técnicas de
    esparsidade e supervisão fraca, desafiamos a doutrina de que
    modelos neuronais não são capazes de exibir tais
    características.

    Para atingir eficiência na utilização de dados, propomos
    aproveitar as capacidades de transferência de aprendizagem de
    redes neuronais pré-treinadas em tarefas sequenciais. Aplicamos o
    nosso método em Pós-Edição Automática (PEA) para demonstrar a sua
    eficácia. Para isso, usamos um modelo neuronal pré-treinado nos
    dois componentes principais do sistema de PEA, explorando várias
    estratégias de compartilhamento de parâmetros. Conseguimos obter
    resultados competitivos com modelos treinados em grandes
    quantidades de dados artificiais, mas treinando em apenas um
    pequeno conjunto de dados.

    Para atingir transparência, propomos substituir a função
    \textit{softmax} nos mecanismos de atenção de redes neuronais
    pela função \textit{$\alpha$-entmax}: uma generalização
    diferenciável da \textit{softmax} que permite que palavras com
    baixa pontuação recebam precisamente um peso de zero. Para além
    disso, derivamos um método para aprender o parâmetro $\alpha$
    automaticamente, que controla a forma e a dispersão da função
    \textit{$\alpha$-entmax}. Em tradução automática, mostramos que a nossa
    abordagem adaptativa melhora a interpretabilidade quando comparado
    à simples aplicação da \textit{softmax}.

    Para atingir modelos compactos, propomos uma nova técnica para
    treinar modelos com variáveis latentes discretas de forma mais
    eficaz. Treinar este tipo de modelos é muitas vezes desafiador devido
    à necessidade de usar estratégias de amostragem com variância alta,
    ou usar relaxamentos para o espaço contínuo. Em vez disso, propomos usar
    mapeamentos esparsos diferenciáveis para parametrizar as
    distribuições discretas. A esparsidade vai permitir uma marginalização eficiente como
    alternativa às técnicas mencionadas.
    Aplicamos o nosso método a três diferentes tarefas de modelos
    com variáveis latentes discretas e obtemos resultados competitivos
    atingindo, mesmo assim, a praticabilidade de aproximações
    baseadas em amostragem.

\end{resumo}
