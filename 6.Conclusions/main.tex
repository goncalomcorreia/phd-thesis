\fancychapter{Conclusions}
\label{chap:conclusions}

\cleardoublepage
\doublespacing

To wrap up the present thesis, we review the main contributions
developed in this work, and discuss their impact, limitations, and
interesting directions for future work.

\section{Summary of Contributions and Discussion}

Over the course of this thesis, we have developed models and
techniques that aim to make neural models more compact, transparent,
and data efficient.

In \chapref{chap:ape}, we achieved \textbf{data efficiency} by
leveraging the \textbf{transfer learning} power of a pre-trained
language model to avoid the need for producing a large synthetic
dataset, in the context of a real-world application: Automatic
Post-Editing.

In \chapref{chap:adaptsparse}, we proposed a model with increased
\textbf{transparency} by letting it \textbf{learn its own sparsity}.
The chosen application was Neural Machine Translation, and we showed
how the sparsity can help to interpret the various roles of each
attention head in the model.

Finally, in \chapref{chap:sparsemarg}, we once again leveraged
\textbf{learnable sparsity} by introducing a new method to train
latent variable models with discrete or structured nodes. We show the
efficacy of our method in several applications: semi-supervised
learning, emergent communication, and unsupervised learning of
bit-vector representations. This method achieves better performance
than standard methodologies for training discrete latent variable
models and as such aids in the development of more \textbf{compact}
neural models.

A common theme in this thesis is sparsity, particularly
sparse probability distributions. We have followed in the footsteps
of work started by \citet{sparsemax} and proposed novel uses of
these sparse distributions and promising alternatives to them.
In all works described, we have shown how, in different applications,
we can let neural models learn how much sparsity they need; be it
in the context of an attention mechanism, or in the context of
the number of relevant latent assignments during training.

On another front, we tackle some forms of \textbf{weak supervision}.
On one hand, we use transfer learning and minimize the amount of data
required to train an APE model in \chapref{chap:ape}. On the other
hand, we also applied our method of \chapref{chap:sparsemarg} to
obtain improved performance on a semi-supervised learning task.
Additionally, in \secref{sec:subsequent_work_adapt} we saw how our
discovered attention head roles can be used to fix attention patterns
in a Transformer, increasing inductive bias in the
process~\citep{raganato2020FixedEncoderSelfAttentiona}.

\section{Open Problems and Limitations}

In the pursuit of diligent research, we briefly discuss some of the
open problems and limitations of the work presented in this thesis.
We seek to not diminish any of these limitations, but rather present
them fairly. We will also discuss the ethical ramifications of our
work in a later section (\secref{sec:impact}).

As previously stated, sparsity is a prominent theme in this work.
That sparsity is achieved through probability normalization functions
such as sparsemax, $\alpha$-\entmaxtext, and SparseMAP. While
sparsity can lead to increased efficiency, we do not fully take
advantage of this in this work, particularly in the publicly released
code. For example, for $\alpha$-\entmaxtext, further speed-ups are
possible by leveraging more parallelism in the bisection algorithm
for computing it.

In \chapref{chap:ape}, we have used a large pre-trained model to
circumvent the need to train MT models to produce a large synthetic
dataset and then train an APE model on it. Although we believe
this to be a way to spare resources, we do not wish to underplay
the resources used to train the pre-trained model itself.

While we have shown promising results in \chapref{chap:sparsemarg},
we are aware that the experiments we conducted were performed on toy
datasets. While this may overemphasize the success of the work, we
have seen in \secref{sec:subsequent} that our method has already been
successfully applied to applications that have real-world datasets.
We hope to continue seeing this trend in the future.

\section{Future Directions}

\begin{itemize}
    \item semi-supervision of latent discrete variables and few-shot learning
    \item non-differentiability of the latent discrete variables learning signal
    \item structured latent variables applications in NLP: enumeration can make it easier to learn
\end{itemize}

\section{Broader Impact}\label{sec:impact}

We discuss here the broader impact of our work. Discussion in this
section is predominantly speculative, as the methods described in
this work are not yet tested in broader applications. However, we do
think that the methods described here can be applied to many
applications\,---\,as this work is applicable to any model that
contains discrete latent variables, even of combinatorial type.

Currently, the solutions available to train discrete latent variable
models (LVMs) greatly rely on MC sampling, which can have high variance.
Methods that aim to decrease this variance are often not trivial to
train and to implement and may disincentivize practitioners from
using this class of models. However, we believe that discrete LVMs
have, in many cases, more interpretable and intuitive
latent representations. Our methods offer: a simple approach in
implementation to train these models; no addition in the number of
parameters; low increase in computational overhead (especially when
compared to more sophisticated methods of variance
reduction~\citep{RB19}); and improved performance. Our code has been
open-sourced as to ensure it's scrutinizable by
anyone and to boost any related future work that other researchers
might want to pursue.

As we have already pointed out, oftentimes LVMs
have superior explanatory power and so can aid in understanding cases
in which the model failed the downstream task. Interpretability of
deep neural models can be essential to better discover any ethically
harmful biases that exist in the data or in the model itself.

On the other hand, the generative models discussed in this work may
also pave the way for malicious use cases, such as is the case with
\emph{Deepfakes}, fake human avatars used by malevolent Twitter
users, and automatically generated fraudulent news. Generative models
are remarkable at sampling new instances of fake data and, with the
power of latent variables, the interpretability discussed before can
be used maliciously to further push harmful biases instead of
removing them. Furthermore, our work is promising in improving the
performance of LVMs with several discrete
variables, that can be trained as attributes to control the sample
generation. Attributes that can be activated or deactivated at will
to generate fake data can both help beneficial and malignant users to
finely control the generated sample. Our work may be currently
agnostic to this, but we recognize the dangers and dedicate effort to
combating any malicious applications.

Energy-wise, LVMs often require less data and
computation than other models that rely on a massive amount of data
and infrastructure. This makes LVMs ideal for
situations where data is scarce, or where there are few computational
resources to train large models. We believe that better latent
variable modeling is a step forward in the direction of alleviating
environmental concerns of deep learning
research~\citep{strubell2019energy}. However, the models proposed in
this work tend to use more resources earlier on in training than
standard methods, and even though in the applications shown they
consume much less as training progresses, it's not clear if that
trend is still observed in all potential applications.

In data science, LVMs, such as
mixed-membership models~\citep{blei2014build}, can be used to uncover
correlations in large amounts of data, for example, by clustering
observations. Training these models requires various degrees of
approximations which are not without consequence, they may impact the
quality of our conclusions and their fealty to the data. For example,
variational inference tends to under-estimate uncertainty and give
very little support to certain less-represented groups of variables.
Where such a model informs decision-makers on matters that affect
lives, these decisions may be based on an incomplete view of the
correlations in the data and/or these correlations may be exaggerated
in harmful ways. On the one hand, our work contributes to more stable
training of LVMs, and thus it is a step towards addressing some of
the many approximations that can blur the picture. On the other hand,
sparsity may exhibit a larger risk of disregarding certain
correlations or groups of observations, and thus contribute to
misinforming the data scientist. At this point it is unclear to which
extent the latter happens and, if it does, whether it is consistent
across LVMs and their various uses. We aim to study this issue
further and work with practitioners to identify failure cases.

\cleardoublepage

\singlespacing