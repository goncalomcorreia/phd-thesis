\fancychapter[Conclusions]{Conclusions}
\label{chap:conclusions}

\cleardoublepage
\doublespacing

\noindent To wrap up the present thesis, we review the main contributions
developed in this work, and discuss their impact, limitations, and
potential directions for future work.

\section{Summary of Contributions}

\noindent Throughout this thesis, we have developed models and
techniques that aim to make neural models more data-efficient, transparent,
and compact.

In \chapref{chap:ape}, we achieved \textbf{data-efficiency} by
leveraging the \textbf{transfer learning} power of a pre-trained
language model to avoid the need for producing a large synthetic
dataset, in the context of a real-world application: Automatic
Post-Editing.

In \chapref{chap:adaptsparse}, we proposed a model with increased
\textbf{transparency} by letting it \textbf{learn its own sparsity}.
The chosen application was NMT, and we showed
how the sparsity can help to interpret the various roles of each
attention head in the model.

Finally, in \chapref{chap:sparsemarg}, we once again leveraged
\textbf{learnable sparsity} by introducing a new method to train
latent variable models with discrete or structured nodes. We show the
efficacy of our method in several applications: semi-supervised
learning, emergent communication, and unsupervised learning of
bit-vector representations. This method achieves better performance
than standard methodologies for training discrete latent variable
models and as such aids in the development of more \textbf{compact}
neural models.

A common theme in this thesis is sparsity, particularly
sparse probability distributions. We have followed in the footsteps
of work started by \citet{sparsemax} and proposed novel uses of
these sparse distributions and promising alternatives to them.
In all works described, we have shown how in different applications
we can let neural models learn how much sparsity they need; be it
in the context of an attention mechanism, or in the context of
the number of relevant latent assignments during training.

On another front, we tackle some forms of \textbf{weak supervision}.
On one hand, we use transfer learning and minimize the amount of data
required to train an APE model in \chapref{chap:ape}. On the other
hand, we also applied our method of \chapref{chap:sparsemarg} to
obtain improved performance on a semi-supervised learning task.
Additionally, in \secref{sec:subsequent_work_adapt} we saw how our
discovered attention head roles can be used to fix attention patterns
in a transformer, increasing inductive bias in the
process~\citep{raganato2020FixedEncoderSelfAttention}.

\section{Open Problems and Limitations}

\noindent In the pursuit of diligent research, we briefly discuss some of the
open problems and limitations of the work presented in this thesis.
We seek not to diminish any of these limitations, but rather present
them fairly.
We will also discuss the ethical ramifications of our
work in a later section (\secref{sec:impact}).

As previously stated, sparsity is a prominent theme in this work.
That sparsity is achieved through probability normalization functions
such as sparsemax, $\alpha$-\entmaxtext, and SparseMAP. While
sparsity can lead to increased efficiency, we do not fully take
advantage of this in the present thesis or in the publicly released
code. For example, for $\alpha$-\entmaxtext, further speed-ups are
possible, with careful engineering, by leveraging more parallelism in
the bisection algorithm for computing it.
This is a relevant research direction, as it is now possible to
exploit recent generations of GPU architectures which are more
sparse-friendly than before.

In \chapref{chap:ape}, we have used a large pre-trained model to
circumvent the need to train MT models to produce a large synthetic
dataset and then train an APE model on it. Although we believe
this to be a way to spare resources, we do not wish to underplay
the resources used to train the pre-trained model itself.

While we have shown promising results in \chapref{chap:sparsemarg},
we are aware that the experiments we conducted were performed on toy
datasets. While this may overemphasize the success of the work, we
have seen in \secref{sec:subsequent} that our method has already been
successfully applied to applications that have real-world datasets.
We hope to continue seeing this trend in the future.

\section{Future Directions}

\noindent We further list in this section exciting directions for future work
that draw inspiration from the methods proposed in this thesis. We
hope this might inspire other researchers in the field to explore
these directions and extend the ideas presented.

\paragraph*{Semi-supervised Learning.} Some forms of weak
supervision were explored in this work. In particular, we have shown
promising results in \chapref{chap:sparsemarg} by introducing a new
method to train discrete latent variable models, which can be used to
train semi-supervised models (\secref{sec:gen}). We believe that our
method can be used in real-world applications where supervision is
limited and that it will lead to better results than standard
methods. For example, in NLP, there are many applications where a
relaxation of the discrete space is not possible~\citep{Lee2019}.
We can consider the semi-supervised loss
%
\begin{equation}
    \mathcal{L}_{x}(\theta) =
    \sum_{z \in \mathcal{Z}} \pi(z | x) \ell(x, z; \theta)
    + \sum_{z \in \mathcal{Z}} \pi(z | x, y) \ell(x, y, z; \theta)
    + \mathcal{R}(\theta)\,,
\end{equation}
%
which is very similar to the loss used in \secref{sec:gen},
and apply it to more challenging datasets.
Whereas before we could only rely on high variance methods if we wished to
train such a model, we can now circumvent this limitation
by using our training method and computing deterministic gradients.

\paragraph*{Non-differentiability of the LVM learning signal.} We
note how, in \eqnref{eq:fit}, the training of the parameters that
learn $\pi(z|x;\theta)$ is decoupled from the training of the
parameters of the generative model of the downstream loss: when
marginalizing the latent variables, the downstream loss only scales
the gradients computed from $\pi(z|x;\theta)$. This means that our
method can still be applied when the downstream loss component does
not have any trainable parameters. Furthermore, that component in
\eqnref{eq:fit} can be replaced by any function, even
non-differentiable, akin to a reward signal. One example of a
real-world application where the downstream loss doesn't have
learnable parameters is the prompting of large pre-trained language
models~\citep{liu2021PretrainPromptPredict}. In this line of work,
the pre-trained model is assumed to already have the knowledge
required to handle the task at hand stored in its parameters and only
a prompt is required to generate the desired output. This problem
could be framed as a latent variable model and one could interpret a
natural language prompt as a discrete latent variable. Using the
property we just described, the downstream task can be optimized via
any differentiable or non-differentiable reward signal
%
\begin{equation}
    \mathcal{L}_{x}(\theta) =
    - \sum_{z \in \mathcal{Z}} \pi(z | x) r(x, z),
\end{equation}
%
where note that $r$ does not depend on the model's parameters $\theta$.

\paragraph*{Latent draft translation.} Related to the overall present
thesis topics of increasing transparency and compactness of neural
models in NLP, we highlight the potential of prototype
editing~\citep{guu2018GeneratingSentencesEditing,
    he2020LearningSparsePrototypes}. In this generative model, the
generative process has two main steps: (1) selecting a prototype
sequence of tokens $\bm{t}$ from a list of possible prototypes (\eg
the training data), and (2) editing that prototype by sampling a latent
edit vector $z$ that encodes the type of edit (\eg active to passive,
reordering, \textit{inter alia}). We note the promising possibility
of using this approach to NMT: While \citep{he2020LearningSparsePrototypes} apply their
sparse neural editor to language modeling, we will model instead a
conditional distribution over $\mathcal{X} \times \mathcal{Y}$:
%
\begin{equation}
    p(y, \bm{t}, \bm{z} | \bm{x}) =
    p(\bm{t} | \bm{x}; \theta)
    p(\bm{z} | \bm{x}; \theta)
    p(\bm{y} | \bm{t}, \bm{z}; \theta)\,.
\end{equation}
%
We may think of t as being a template for y\,---\,it can be a
lexicalized rule with gaps or variables, a phrase-based mapping
from a section of $\bm x$ to a string in the target language, among others.
Once again, for deterministic gradients and greater training stability, we
may use the training method proposed in \chapref{chap:sparsemarg}.

\section{Broader Impact}\label{sec:impact}

\noindent We discuss here the broader impact of our work.

First of all, we note that all of our code has been
open-sourced to ensure it's scrutinizable by
anyone and to boost any related future work that other researchers
might want to pursue.

A common theme in this thesis is transparency.
The main objective of improving such property in neural models
is to have superior explanatory power and therefore to aid in understanding cases
in which the model failed the downstream task. Interpretability of
deep neural models can be essential to better discover any ethically
harmful biases that exist in the data or in the model itself.

On the other hand, the models discussed in this work may
also pave the way for malicious use cases, such as is the case for generative models with
\emph{Deepfakes}, fake human avatars used by malevolent Twitter
users, or generation models that automatically generate fraudulent news,
often based on the large pre-trained language models used in this thesis.
Particularly, generative models as the ones used in \chapref{chap:sparsemarg}
are remarkable at sampling new instances of fake data and, with the
power of latent variables, the interpretability discussed before can
be used maliciously to further push harmful biases instead of
removing them. Furthermore, our work in \chapref{chap:sparsemarg} is promising in improving the
performance of latent variable models with several discrete
variables, that can be trained as attributes to control the sample
generation. Attributes that can be activated or deactivated at will
to generate fake data can both help beneficial and malignant users to
finely control the generated sample. Our work may be currently
agnostic to this, but we recognize the dangers and encourage effort to
combating any malicious applications.

Energy-wise, our data-efficient and compact models aim to require
less data and computation than other models that rely on a massive
amount of data and infrastructure. This makes our models ideal for
situations where data is scarce, or where there are few computational
resources to train large models. We believe that data efficiency and
compactness are a step forward in the direction of alleviating
environmental concerns of deep learning
research~\citep{Strubell2019}. However, we note that, for
example, the adaptive sparsity of the models proposed in
\chapref{chap:sparsemarg} tend to use more resources earlier on
than standard methods. Even though in the applications shown
they consume much less as training progresses, it's not clear if that
trend is still observed in all potential applications.

Regarding sparsity, we note how sparsity may exhibit a larger risk of
disregarding certain correlations or groups of observations,
and thus may contribute to misinforming a practitioner.
Where such a model that uses sparsity informs decision-makers on matters that affect
lives, these decisions may be based on an incomplete view of the
correlations in the data and/or these correlations may be exaggerated
in harmful ways.
At this point, it is
unclear to which extent this happens and, if it does, whether it is
consistent across various uses. We believe that a better understanding
of the impact of sparsity in potentially oversimplifying correlations is an
additional interesting avenue of future work.


\cleardoublepage

\singlespacing