\section{Proposal Outline}
\label{sec:int_outline}

Herein we show the outline of this thesis proposal.

\paragraph*{Chapter 1: Introduction} The research problems are
described and the respective proposed solutions are first introduced.
A work plan for the remaining time of the PhD is shown.

\paragraph*{Chapter 2: Background} An overview of relevant previous
work is presented, covering the basics required to understand
the content of the proposed solutions.

\paragraph*{Chapter 3: A Simple and Effective Approach to Automatic
Post-Editing with Transfer Learning} A sequence-to-sequence model for
Automatic Post-Editing using the Transformer architecture and
harnessing the transfer learning power of BERT~\citep{devlin2018bert}
is proposed.

\paragraph*{Chapter 4: Adaptively Sparse Transformers} Proposes to
augment the Transformer to use attention modules that allow sparse
probabilities that adaptively change their own sparsity depending on
their role within the model.

\paragraph*{Chapter 5: Efficient Marginalization of Discrete and
Structured Latent Variables via Sparsity} A new training method of
discrete and structured neural latent models is proposed, which uses
sparse probabilities in order to compute the training objective of
these models, instead of Monte Carlo methods.

\paragraph*{Chapter 6: Future Work} The proposed work for the
remaining time of the PhD is shown, which include NMT models that
construct a editable draft translation via a latent variable, and
NMT models with latent summaries of the context.