\section{Roadmap}
\label{sec:int_roadmap}

\noindent Herein, we show the outline of the remainder of this thesis.

We begin in \chapref{chap:background} by reviewing major concepts
that are essential to understand the content of this thesis,
particularly neural network models for NLP, sparse probability
normalization functions, and latent variable models.

In \chapref{chap:ape}, we develop a sequence-to-sequence model for
Automatic Post-Editing using the transformer architecture and by
harnessing the transfer learning power of a pre-trained large
language model.

In \chapref{chap:adaptsparse}, we modify the transformer architecture to use
attention modules that allow sparse coefficients that can adaptively
change their sparsity depending on their role within the model.

In \chapref{chap:sparsemarg}, we propose a new training method for
structured and unstructured neural latent variable models, which uses
sparsity to compute the training objective of these
models, instead of using Monte Carlo methods or continuous relaxations.

Finally, in \chapref{chap:conclusions}, we summarize the
contributions of the present thesis, address some of the limitations
and open problems of the present work, and discuss exciting future
directions of further research.
