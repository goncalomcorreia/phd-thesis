\section{Roadmap}
\label{sec:int_roadmap}

Herein we show the outline of the remainder of this thesis.

We begin in \chapref{chap:background} by reviewing major concepts
that are essential to understand the content of this thesis,
particularly neural network models for NLP, sparse probability
normalization functions, and latent variable models.

In \chapref{chap:ape}, we develop a sequence-to-sequence model for
Automatic Post-Editing using the Transformer architecture and by
harnessing the transfer learning power of
BERT~\citep{devlin2018bert}.

In \chapref{chap:adaptsparse}, we augment the Transformer to use
attention modules that allow sparse probabilities that adaptively
change their own sparsity depending on their role within the model.

In \chapref{chap:sparsemarg}, we propose a new training method of
discrete and structured neural latent variable models, which uses
sparse probabilities in order to compute the training objective of
these models, instead of using Monte Carlo methods.

Finally, in \chapref{chap:conclusions}, we summarize the
contributions of the present thesis, address some of the limitations
and open problems of the present work, and discuss exciting future
directions of further research.
