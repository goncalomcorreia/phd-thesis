\section{Contributions and Thesis Statement}
\label{sec:int_contributions}

We will now summarize the contributions of this thesis, which will
address the open questions left to answer in the previous section.

\begin{itemize}

      \item \textbf{We use weak supervision by leveraging transfer learning
                  for data efficient sequence-to-sequence models.}
            We show how to leverage a pre-trained large Transformer
            encoder to perform a sequence-to-sequence task on a very
            small dataset. We explore different avenues of parameter
            sharing and initialization in order to make this
            possible.


      \item \textbf{We propose {\boldmath $\alpha$}-\entmaxtext{} with
                  learnable {\boldmath $\alpha$}, a new sparse probability
                  normalization function that learns its own sparsity.}
            We derive the gradient of $\alpha$ in
            $\alpha$-\entmaxtext{}~\citep{entmax}. By letting a
            Transformer learn through this gradient the $\alpha$ of
            each attention head, the attention heads can dynamically
            change their own sparsity during training. This way, each
            attention head can accommodate its sparsity to the role
            it will play in the overall model.

      \item \textbf{We conduct an extensive analysis on the increased
                  transparency of Transformer models that use {\boldmath
                              $\alpha$}-\entmaxtext{} as its normalization function in the
                  attention mechanism.}
            Besides studying the distribution of sparsity and respective $\alpha$ values
            throughout all attention heads in the Transformer, we also identify
            examples of sharper attention head behavior than what was found in
            previous work, along with the disentanglement of new found behaviors
            thanks to our proposed sparsity.

      \item \textbf{We propose a new method to train discrete and
                  structured latent variable models, based on the
                  efficiency of marginalizing expectations using
                  sparsity.}
            Thanks to this method, we can train these families of
            latent variable models without recurring to any Monte Carlo
            estimation or relaxations of the discreteness into the
            continuous space. In particular, in the unstructured
            case, our method relies on the sparsemax activation
            function. In the structured case, we propose to either
            use SparseMAP~\citep{niculae2018sparsemap} or the
            novel top-$k$ sparsemax.

      \item \textbf{We provide open-source code for each of the
                  methods we have proposed.}
            The respective repositories can be found in each
            of the chapters.


\end{itemize}

\paragraph{Thesis Statement.} The primary claim of this thesis is
that, unlike what is insisted in numerous previous work, neural
models have the capability of being data-efficient, transparent, and
compact: one only needs to look through a different lens that is
capable of leveraging weak supervision, sparsity, and latent
representations. We come to the conclusion that a \textit{vanilla}
application of neural models to the problem at hand is not sufficient
to have these properties: for \textbf{data-efficiency}, we find that we need
to allow parameter sharing, careful initialization, and powerful
transfer learning capabilities in order to succeed at a low-resource
generation task; for \textbf{transparency}, we can allow the model to have
sparsity and learn it according to its needs at training time; and
for \textbf{compactness}, we can leverage discrete latent variable models in a
better way than what was previously possible by using an exact but
efficient gradient.
