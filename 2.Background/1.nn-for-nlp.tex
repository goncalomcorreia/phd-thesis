\section{Neural Networks for NLP}
\label{sec:nmt}

\subsection{Foundations}

\subsection{Sequence-to-Sequence Models}
\label{sec:transformer_bg}

Sequence-to-sequence (seq2seq) models are the broad, main family of models
used to train NMT systems. While there is a plethora of different architectures,
seq2seq models have in common the following blocks:

\begin{itemize}
    \item the {\bf encoder}, which takes as input a sequence $x=[x^1,
              \dots, x^N]$ and creates hidden representations $h=[h^1, \dots,
              h^N]$, by learning representations of each word in $x$ and then
          representations that take into account the context around each word;
    \item the {\bf decoder}, which takes the hidden representations
          $h$ of the encoder and learns a mapping to produce a
          variable-length sequence $\hat{y}$ conditioned on $h$, by
          learning from the true target sentence $y=[y^1, \dots, y^M]$.
\end{itemize}

In the following section, we will expand on the current
state-of-the-art seq2seq architecture.

\subsection{Transformer}

The Transformer~\citep{vaswani2017attention} is a
seq2seq model which maps an input sequence to
an output sequence through many layers of \textbf{multi-head
    attention} mechanisms, yielding a dynamic, context-dependent strategy
for propagating information within and across sentences. It contrasts
with previous seq2seq models, which usually rely either on costly
gated recurrent operations~\citep[often
    LSTMs:][]{bahdanau2014neural,luong2015effective} or static
convolutions~\citep{convseq}.

Given $n$ query contexts and $m$ sequence items under consideration,
attention mechanisms compute, for each query, a weighted
representation of the items. The particular attention mechanism used
in \citet{vaswani2017attention} is called \emph{scaled dot-product
    attention}, and it is computed in the following way:
%
\begin{equation}
    \att(\bm{Q}, \bm{K}, \bm{V}) = \amap
    \left(\frac{\bm{Q}\bm{K}^\tr}{\sqrt{d}}\right) \bm{V},
    \label{eq:att_scaled_dot}
\end{equation}
%
where $\bm{Q} \in \reals^{n \times d}$ contains representations of the
queries, $\bm{K}, \bm{V} \in \reals^{m \times d}$
are the \emph{keys} and \emph{values} of the items attended over,
and $d$ is the dimensionality of these
representations.
The $\amap$ mapping normalizes row-wise using \textbf{softmax},
$\amap(\bm{Z})_{ij} = \softmax(\bm{z}_i)_j$, where
%
\begin{equation}
    \softmax(\bm{z}) = \frac{\exp(z_j)}{\sum_{j'} \exp(z_{j'})}.
\end{equation}
%
In words, the \emph{keys} are used to compute a relevance score
between each item and query. Then, normalized attention weights are computed
using softmax, and these are used to weight the \emph{values} of each item at each
query context.

\begin{definition}
    The normalization function \textbf{softmax} is defined as
    \begin{equation}\label{eq:softmax}
        \softmax(\bm{z}) = \frac{\exp(z_j)}{\sum_{j'} \exp(z_{j'})}.
    \end{equation}
\end{definition}

However, for complex tasks, different parts of a sequence may be relevant in
different ways, motivating \emph{multi-head attention} in transformers.
This is simply the application of
\eqnref{eq:att_scaled_dot} in parallel $H$ times, each with a different,
learned linear transformation that allows specialization:
%
\begin{equation}\label{eq:head}%
    \hspace{-.01ex}%
    \ath_i(\bm{Q}\!,\!\bm{K}\!,\!\bm{V})\!=\!\att(\bm{QW}_i^Q\!\!,\bm{KW}_i^K\!\!,\bm{VW}_i^V\!)%
    \hspace{-1.5ex}%
\end{equation}
%
In the Transformer, there are three separate multi-head attention mechanisms for
distinct purposes:
\begin{itemize}
    \item \textbf{Encoder self-attention:} builds rich, layered representations of
          each input word, by attending on the entire input sentence.
    \item \textbf{Context attention:} selects
          a representative weighted average of the encodings of the input words, at each
          time step of the decoder.
    \item \textbf{Decoder self-attention:} attends over the partial output sentence
          fragment produced so far.
\end{itemize}
Together, these mechanisms enable the contextualized flow of information between
the input sentence and the sequential decoder.

\subsection{Large Pre-trained Language Models}
