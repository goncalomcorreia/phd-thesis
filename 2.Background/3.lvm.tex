\section{Latent Variable Models}
\label{sec:lvm}

\noindent In \chapref{chap:sparsemarg}, we will focus on making
neural models more compact. To that effect, we propose a novel way to
train latent variable models. Latent variable models differ from the
models described in \secref{sec:ml-primer}, which only have
\textit{observed variables} ($X$ and $Y$). Latent variables are
variables that are not observed (\ie, \textit{hidden}) but are assumed
to be correlated with the observed variables. Even though the
training of these models may be difficult, there are significant
advantages to this approach. Particularly, these models often need
fewer parameters, making them more compact. Another source of
compactness is that these models have built-in \textit{inductive
    bias}, that is, they make assumptions about how the variables
interact, yielding better interpretability and more compact
representations.

Mathematically, the possibility of a latent variable being present
in the modeling of $p_X(\bm{x}|\theta)$ stems from the simple
observation that
%
\begin{equation}\label{eq:lvm}
    \log p_X(\bm{x}|\theta) =
    \mathbb{E}_{\pi(z | x; \theta)} \left[\log p_X(\bm{x}|\theta)\right]\,,
\end{equation}
%
where $z$ is the underlying latent or hidden variable and $\pi$ is the
probability of an assignment $z$ given the current observed $\bm{x}$.

When discussing latent variable models, we assume throughout that there are observed variables
$x \in \mathcal{X}$ and latent stochastic variables $z \in \ZZ$. The
overall fit to a dataset $\mathcal D$ is $L_\mathcal{D}(\theta) =
    \sum_{x \in \mathcal{D}} \LL_x(\theta)$, where the loss of each
observation,
%
\begin{equation}\label{eq:fit}
    \mathcal{L}_{x}(\theta) =
    \mathbb E_{\pi(z|x; \theta)}
    \left[ \ell(x, z; \theta)\right] =
    \sum_{z \in \mathcal Z} \pi(z | x; \theta)~\ell(x, z; \theta) ~,
\end{equation}
%
is the expected value of a downstream loss $\ell(x,z;\theta)$\footnote{
    We swapped the probabilistically-grounded $\log p_X(\bm{x}|\theta)$ in \eqnref{eq:lvm}
    for a more general $\ell(x,z;\theta)$ in order
    to englobate any possible loss, probabilistic or non-probabilistic.
}
under a
probability model $\pi(z|x;\theta)$ of the latent variable; in other
words, the latent variable $z$ is {\it marginalized} to compute this loss.
\eqnref{eq:fit} might also include a regularizer $\mathcal{R}(\theta)$.
With this loss, we are assuming that the latent variable is discrete. We refer the reader
to \citet{Kim2018} for other possibilities, such as continuous latent variables, and more
details in general.
To model complex data, one parameterizes both the downstream loss and
the pmf over latent assignments using neural networks, due
to their flexibility and capacity~\citep{Kingma+2014:VAE}.

\subsection{Discrete Latent Variables}\label{sec:discrete_lvm_bg}

\noindent In this thesis, we will focus on \textbf{discrete} latent variables, where
$|\ZZ|$ is finite but possibly very large. One example is when
$\pi(z|x;\theta)$ specifies a Categorical distribution, parameterized by a
vector $\pv \in \simplex^{|\ZZ|-1}$. To obtain $\pv$, a neural network
computes a vector of scores $\s \in \mathbb{R}^{|\ZZ|}$, one score
for each assignment, which is then mapped to the probability simplex,
typically via $\pv = \softmax(\s)$. Another example is when $\ZZ$ is
a structured (combinatorial) set, such as $\ZZ = \{0, 1\}^D$. In this
case, $|\ZZ|$ grows exponentially with $D$ and it is infeasible to
enumerate and score all possible assignments. For this structured
case, scoring assignments involves decomposition into parts, which
we describe in \secref{sec:struct_lvm_bg}.

Training such models requires summing the contributions of all
assignments of the latent variable, which involves as many as $|\ZZ|$
evaluations of the downstream loss. When $\ZZ$ is not too large, the
expectation may be evaluated explicitly, and learning can proceed
with exact gradient updates. If $\ZZ$ is large, and/or if $\ell$ is
an expensive computation, evaluating the expectation becomes
prohibitive. In such cases, practitioners typically turn to MC
estimates of $\nabla_\theta \LL_x(\theta)$ derived from latent
assignments sampled from $\pi(z|x;\theta)$. Under an appropriate
learning rate schedule, this procedure converges to a local optimum
of $\mathcal L_x(\theta)$ as long as gradient estimates are unbiased
\citep{robbins1951stochastic}. Next, we describe the two current main
strategies for MC estimation of this gradient. Later, in
\chapref{chap:sparsemarg}, we propose our
\textbf{deterministic} alternative, based on sparsifying $\pi(z| x,
    \theta)$.

\paragraph*{Monte Carlo gradient estimates.} Let $\theta=(\theta_\pi,
    \theta_\ell)$, where $\theta_\pi$ is the subset of weights that $\pi$
depends on, and $\theta_\ell$ the subset of weights that $\ell$
depends on. Given a sample $\zsamp \sim \pi(z| x;\theta_\pi)$, an
unbiased estimator of the gradient for \eqnref{eq:fit} \wrt
$\theta_\ell$ is $\nabla_{\theta_\ell} \mathcal L_x(\theta) \approx
    \nabla_{\theta_\ell} \ell(x, \zsamp; \theta_\ell)$. Unbiased
estimation of $\nabla_{\theta_\pi} \mathcal L_x(\theta)$ is more
difficult, since $\theta_\pi$ is involved in the sampling of
$\zsamp$, but can be done with the Score Function
Estimator~\citep[SFE;][]{rubinstein1976monte,paisley2012variational}:
$\nabla_{\theta_\pi} \mathcal L_x(\theta) \approx \ell(x, \zsamp;
    \theta_\ell) ~ \nabla_{\theta_\pi} \log \pi(\zsamp | x; \theta_\pi)$,
also known as \textsc{reinforce}~\citep{Williams1992}. The SFE is
powerful and general, making no assumptions on the form of $z$ or
$\ell$, requiring only a sampling oracle and a way to assess
gradients of $\log \pi(\zsamp | x; \theta_\pi)$. However, it comes
with the cost of high variance. Making the estimator practically
useful requires variance reduction techniques such as
baselines~\citep{Williams1992,MuProp} and control
variates~\citep{CV2013,REBAR,RELAX}. Variance reduction can also be
achieved with Rao-Blackwellization techniques such as sum and
sample~\citep{casella1996rao,BBVI14,RB19}, which marginalizes an
expectation over the top-$k$ elements of $\pi(z| x;\theta_\pi)$ and
takes a sample estimate from the complement set.

\paragraph*{Continuous relaxations.} For continuous latent variables,
low-variance pathwise gradient estimators can be obtained by
separating the source of stochasticity from the sampling parameters,
using the so-called \emph{reparameterization trick}
\citep{Kingma+2014:VAE,RezendeEtAl14VAE}. For discrete latent
variables, reparameterizations can only be obtained by introducing a
step function like $\argmax$, which has null gradients almost everywhere.
Replacing the gradient of $\argmax$ with a nonzero surrogate like the identity
function, known as Straight-Through~\citep{STE}, or with the gradient of softmax, known
as \emph{Gumbel-Softmax}~\citep{Concrete,GumbelSoftmax}, leads to a
biased estimator that can still perform well in practice. Continuous
relaxations like Straight-Through and Gumbel-Softmax are only
possible under a further modeling assumption that $\ell$ is defined
continuously (thus differentiably) in a neighborhood of the
indicator vector $\bm{z} = \bm{e}_{\zsamp}$ for every $z \in \ZZ$.

\subsection{Structured Latent Variables}\label{sec:struct_lvm_bg}

\noindent Many models of interest involve structured (or combinatorial) latent
variables. In this thesis, we assume that a latent $z$ can be represented as a
    {\it bit-vector}\,---\,\ie, a vector of discrete binary variables
$\bm{a}_{z} \in \{0, 1\}^D$. This assignment of binary variables may
involve global factors and constraints (\eg, tree constraints, or
budget constraints on the number of active variables, \ie, $\sum_i
    [\bm{a}_{z}]_i \le B$, where $B$ is the maximum number of variables
allowed to activate at the same time). In such structured problems,
$|\ZZ|$ increases exponentially with $D$, making an exact evaluation of all possible
$\ell(x, z; \theta)$ prohibitive.

Structured prediction typically handles this combinatorial explosion
by parameterizing scores for individual binary variables and
interactions within the global structured configuration, yielding a
compact vector of \textbf{variable scores} $\bm{t} = \g(x; \theta)
    \in \mathbb{R}^D$ (\eg, log-potentials for binary attributes), with
$D \ll |\ZZ|$. Then, the score of some global configuration $z \in
    \ZZ$ is $s_z \defeq \DP{\bm{a}_z}{\bm{t}}$. The variable scores
induce a unique Gibbs distribution over structures, given by
$\pi(z|x;\theta) \propto \exp(\DP{\bm{a}_{z}}{\bm{t}})$.
Equivalently, defining $\bm{A} \in \mathbb{R}^{D \times |\ZZ|}$ as
the matrix with columns $\bm{a}_{z}$ for all $z \in \ZZ$, we consider
the vector of probabilities parameterized by $\softmax(\s)$, where
$\s=\bm{A}^\top \bm{t}$. (In the unstructured case, $\bm{A}=\bm{I}$.)

In practice, however, we cannot materialize the matrix $\AA$ or the
global score vector $\bm{s}$, let alone compute the softmax and the
sum in \eqnref{eq:fit}. The SFE, however, can still be used, provided
that exact sampling of $z\sim\pi(z | x; \theta)$ is feasible, and
efficient algorithms exist for computing the normalizing constant
$\sum_{z'}\exp(\DP{\bm{a}_{z'}}{\bm{t}})$~\citep{WJ2008}, needed to
compute the probability of a given sample.
