\section{Sparsity and the Simplex}
\label{sec:sparsity_background}

% TODO: in attention, it's coefficients of a convex combination

\noindent We now shift our attention to projections onto the \textbf{simplex}. The
\textit{probability simplex} $\simplex^K$ is a mathematical space
such that, if $\bm{x}\in\simplex^K$, then $0 \leq x_k \leq 1$ for any
$k\in\left\{0, \dots, K\right\}$ and $\sum_{k=0}^K x_k = 1$. As
aforementioned, \textbf{softmax} is one such projection into it
(\eqnref{eq:softmax}), but others exist, which we will explore and use in this thesis.

Note how the softmax mapping (\eqnref{eq:softmax}) is elementwise
proportional to $\exp$; therefore, it can never assign a weight of
\textbf{exactly zero}. Thus, unnecessary items are still taken into
consideration to some extent. Since its output sums to one, this
invariably means less weight is assigned to the relevant items,
potentially harming performance and
interpretability~\citep{jain2019attention}. This has motivated a line
of research on learning networks with \emph{sparse}
mappings~\citep{sparsemax,fusedmax,louizos,shao2019ssn}. In this
section, we will explore projections that will be used or expanded in
this thesis: sparsemax~\citep{martins2016softmax},
$\alpha$-\entmaxtext~\citep{blondel2019learning,entmax}, and
SparseMAP~\cite{niculae2018sparsemap}.

\subsection{Sparsemax}\label{sec:sparsemax_bg}

\noindent Firstly, we will introduce the \textbf{sparsemax} mapping.
Like softmax, sparsemax is differentiable and has efficient forward and backward
passes~\citep{Held1974,martins2016softmax}, described in detail below.

\begin{definition}[sparsemax]
    The sparsemax transformation
    mapping~\citep{martins2016softmax}, is given by the unique solution
    to the convex optimization problem
    %
    \begin{equation}\label{eq:sparsemax}
        \sparsemax(\s) \coloneqq \argmin_{\pv \in \simplex^K} \| \pv - \s \|_2^2\,.
    \end{equation}
\end{definition}

Since
\eqnref{eq:sparsemax} is the Euclidean projection operator onto the probability
simplex and solutions can hit the boundary, sparsemax is likely to assign {\bf
        probabilities of exactly zero}; in contrast, softmax is always dense.

As a projection onto a polytope, the solution is likely to fall on the
set's boundaries or corners. In this case, points on the border of
$\simplex^K$ have one or more zero coordinates. In contrast, $\softmax(\s)
    \propto \exp(\s)$ is always strictly inside the simplex.
From the optimality conditions of the sparsemax problem shown in \eqnref{eq:sparsemax},
it follows that the solution must have the form:

\begin{lemma}[Proof in \citet{sparsemax}]
    \begin{equation}\label{eq:sparsemax_form}
        \sparsemax(\s) = \max(\s - \tau, 0)\,,
    \end{equation}
    %
    where the maximum is elementwise, and $\tau$ is the unique value that
    ensures the result sums to one.
\end{lemma}

Letting $\bar{\ZZ}$ be the set of nonzero coordinates in the solution, \ie the \textbf{support},
the normalization condition is equivalently
%
\begin{equation}
    \tau = \frac{\sum_{z \in \bar{\ZZ}} \ss_z}{|\bar{\ZZ}|}\,.
\end{equation}
%
Observing that small changes to $\s$ almost always have no effect on the support
$\bar{\ZZ}$, differentiating Equation~\ref{eq:sparsemax_form} gives
\begin{equation}
    \pfrac{\bar{\pv}}{\bar{\s}} = \bm{I}_{|\bar{\ZZ}|} - \frac{1}{|\bar{\ZZ}|}
    \bm{11}^\top\,,
\end{equation}
where $\bar{\pv}$ and ${\bar{\s}}$ denote the subsets of the respective vectors
indexed by the support $\bar{\ZZ}$. Outside of the support, the partial
derivatives are zero. (\emph{Cf.} the more general result in \citet[Proposition
    2]{entmax}.)

In terms of computation, $\tau$ may be found numerically using root finding
algorithms on $f(\tau) = \max(\s - \tau, 0) - 1$.
Alternatively, observe that it is enough to find $\bar{\ZZ}$. By showing that
sparsemax must preserve the ordering, \ie that if $\ss_{z'} > \ss_z$ and $z \in
    \bar{\ZZ}$ then $z' \in \bar{\ZZ}$, it can be shown that $\bar{\ZZ}$ must
consist of the $k$ highest-scoring coordinates of $\s$, where $k$ can be find by
inspection after sorting $\s$. This leads to a straightforward $\mathcal{O}(K
    \log K)$ algorithm due to Held~\citep[pp.~16--17]{Held1974}. This can be further pushed to
$\mathcal{O}(K)$ using median pivoting algorithms~\citep{Condat2016, entmax}. We use
a simpler implementation based on repeatedly calling $\topk$,
doubling $k$ until the optimal solution is found. Since solutions get sparser
over time and $\topk$ is GPU-accelerated
in modern libraries~\citep{pytorch}, this strategy is very fast in practice.

\subsection{Entmax}\label{sec:entmax_bg}

We now focus on $\alpha$-\entmaxtext~\citep{blondel2019learning,entmax}.

\newpage

\begin{definition}[$\aentmax$]
    The $\alpha$-\entmaxtext~\citep{blondel2019learning,entmax} mapping
    is given by the unique solution to the convex optimization problem
    \begin{equation}\label{eq:define_entmax}
        \aentmax(\bm{z}) \coloneqq
        \argmax_{\p \in \simplex^K} \bm{p}^\top\bm{z} + \HHt_{\alpha}(\bm{p}),
    \end{equation}
    %
    where for $\alpha\geq1$,
    $\HHt_\alpha$ is the Tsallis continuous family of entropies
    \citep{Tsallis1988}:
    %
    \begin{equation}\label{eq:tsallisdef}
        \HHt_{\alpha}(\bm{p})\!\coloneqq\!
        \begin{cases}
            \frac{1}{\alpha(\alpha-1)}\sum_j\!\left(p_j - p_j^\alpha\right)\!, &
            \!\!\!\alpha \ne 1,                                                  \\
            -\sum_j \pp_j \log \pp_j,                                          &
            \!\!\!\alpha = 1.
        \end{cases}
    \end{equation}
    %
    This family contains the well-known Shannon and Gini entropies,
    corresponding to the cases $\alpha=1$ and $\alpha=2$, respectively.
\end{definition}

\eqnref{eq:define_entmax} involves a convex optimization subproblem. Using the
definition of $\HHt_\alpha$, the optimality conditions may be used to derive its solution:

\begin{lemma}[Proof in \citet{entmax}]
    \label{lemma:tsallis_reduction}%
    For any $\x$, there exists a unique $\tau^\star$ such that
    %
    \begin{equation}\label{eq:entmax_form}
        \aentmax(\x)
        = [(\alpha - 1){\x} - \tau^\star \ones]_+^{\nicefrac{1}{\alpha-1}}.
    \end{equation}
    %
    where $[\cdot]_+$ is the positive part (ReLU) function, $\bm{1}$
    denotes the vector of all ones, and $\thresh$---which acts like a
    threshold---is the Lagrange multiplier corresponding to the $\sum_i
        \pp_i=1$ constraint.
\end{lemma}
% \begin{proof}
%     From the definition of $\aentmax$,
%     \begin{equation}
%         \aentmax(\bm{z}) \coloneqq
%         \argmax_{\p \in \simplex^d} \bm{p}^\top\bm{z} + \HHt_{\alpha}(\bm{p}),
%     \end{equation}
%     we may easily identify it with a regularized prediction function~\citep{blondel2019learning}:
%     \[\aentmax(\x) \equiv \amap_{-\HHt_{\alpha}}(\x).\]
%     We first note that for all $\p \in \triangle^d$,
%     \begin{equation}
%         -(\alpha-1) \HHt_\alpha(\p) = \frac{1}{\alpha} \sum_{i=1}^d p_i^\alpha + \text{const}.
%     \end{equation}
%     From the constant invariance and
%     scaling properties of $\amap_{\Omega}$
%     \citep[Proposition~1, items~4--5]{blondel2019learning},
%     \[\amap_{-\HHt_\alpha}(\x)
%         = \amap_{\Omega}((\alpha - 1)\x),
%         \quad\text{with}\quad
%         \Omega(\p) =
%         \sum_{j=1}^{d} g(\pp_j),
%         \quad
%         g(t) = \frac{t^\alpha}{\alpha}.
%     \]
%     Using \citet[Proposition~5]{blondel2019learning}, noting that
%     $g'(t) = t^{\alpha - 1}$ and $(g')^{-1}(u) =  u^{\nicefrac{1}{\alpha-1}}$,
%     yields
%     \begin{equation}\label{eq:rootform}
%         \amap_{\Omega}(\x) = [\x - \tau^\star \ones]_+^{\nicefrac{1}{\alpha-1}},
%         \quad\text{and therefore}\quad
%         \aentmax(\x) = [(\alpha-1)\x - \tau^\star \ones]_+^{\nicefrac{1}{\alpha-1}}.
%     \end{equation}
%     Since $\HHt_\alpha$ is strictly convex on the simplex, $\aentmax$ has a unique
%     solution $\p^\star$. \eqnref{eq:rootform}
%     implicitly defines a one-to-one mapping between $\p^\star$ and $\tau^\star$
%     as long as $\p^\star \in \simplex$,
%     therefore $\tau^\star$ is also unique.
% \end{proof}

\paragraph*{Properties of {\boldmath $\alpha$}-\entmaxtext.}
The appeal of $\alpha$-\entmaxtext for attention rests on the
following properties. For $\alpha=1$, \ie when $\HHt_\alpha$ becomes
the Shannon entropy, it exactly recovers the softmax mapping (We
provide a short derivation in \appref{app:fenchelyoung}). For all $\alpha>1$
it permits sparse solutions, in stark contrast to softmax. In
particular, for $\alpha=2$, it recovers the sparsemax mapping
\citep{sparsemax}, which is piecewise linear. In-between, as $\alpha$
increases, the mapping continuously gets sparser as its curvature
changes.

To compute the value of $\alpha$-\entmaxtext, one must find the
threshold $\thresh$ such that the \rhs in \eqnref{eq:entmax_form} sums
to one. \citet{blondel2019learning} propose a general bisection
algorithm. \citet{entmax} introduce a faster, exact algorithm for
$\alpha=1.5$, and enable using $\aentmax$ with fixed $\alpha$ within
a neural network by showing that the $\alpha$-\entmaxtext Jacobian
\wrt $\x$ for $\p^\star = \aentmax(\x)$ is

\begin{equation}
    \begin{aligned}
        \pfrac{\aentmax(\x)}{\x} = \diag(\bm{s}) - \frac{1}{\sum_j s_j} \bm{ss}^\top,
        \text{where}\quad s_i = \begin{cases}(\pp_i^\star)^{2-\alpha}, & \pp_i^\star > 0, \\
             0,                        & \pp_i^\star = 0. \\\end{cases}
    \end{aligned}
\end{equation}

Our work in \secref{sec:adaptive} furthers the study of
$\alpha$-\entmaxtext by providing a derivation of the Jacobian {\bf
        \wrt the parameter} $\boldsymbol{\alpha}$, thereby allowing the shape
and sparsity of the mapping to be learned automatically. This is
particularly appealing in the context of multi-head attention
mechanisms, where we shall show in \secref{sec:stats} that different
heads tend to learn different sparsity behaviors.

\paragraph*{Connections to softmax and sparsemax.}\label{sec:softmax}
The solutionof sparsemax can be characterized through \eqnref{eq:sparsemax_form}
and thus each coordinate of the sparsemax solution is a piecewise-linear function.
Visibly, this expression is recovered when setting $\alpha=2$ in the
$\alpha$-\entmaxtext expression (\eqnref{eq:entmax_form}); for other
values of $\alpha$, the exponent induces curvature.

On the other hand, softmax can be shown to be the unique solution of the optimization problem
\begin{equation}
    \softmax(\x)_i =
    \argmax_{\p \in \simplex} \p^\top\x + \HHs(\p),
\end{equation}
where $\HHs(\p) \coloneqq -\sum_i \pp_i \log \pp_i$ is the Shannon entropy.
Indeed, setting the gradient to $0$ yields the condition
$\log \pp_i = \xx_j - \nu_i - \tau - 1$, where $\thresh$ and $\nu > 0$ are Lagrange
multipliers for the simplex constraints $\sum_i \pp_i = 1$ and $\pp_i \geq 0$,
respectively. Since the \lhs is only finite for $\pp_i>0$,
we must have $\nu_i=0$ for all $i$, by complementary
slackness. Thus, the solution must have the form $\pp_i =
    \nicefrac{\exp(\xx_i)}{Z}$, yielding \eqnref{eq:softmax}.

\subsection{SparseMAP}\label{sec:smap_bg}

\noindent Theoretically, the approaches described in \secref{sec:sparsemax_bg}
and \secref{sec:entmax_bg} apply to any value of $K$ in the simplex $\simplex^K$, but many
interesting applications involve a $K$ with
combinatorial size. For such cases, we turn to structured prediction
methods to handle the combinatorial explosion. Of particular
interest in this thesis is \smap~\citep{niculae2018sparsemap,
    sparsemapcg}, a structured extension of sparsemax.

\begin{definition}[SparseMAP]
    SparseMAP~\citep{niculae2018sparsemap,
        sparsemapcg} is given by the solution of the optimization problem
    \begin{equation}\label{eq:sparsemap}
        \smapop(\bm{t}) \defeq \argmin_{\pv \in \simplex^{|\ZZ|}}
        \|\bm{A}\pv - \bm{t}\|_2^2.
    \end{equation}
\end{definition}

\smap has been used successfully to model structures such as trees
and matchings, and \citet{niculae2018sparsemap} apply an active set
algorithm for evaluating it and computing gradients efficiently,
requiring only a primitive for computing $\argmax_{z \in \ZZ} \langle
    \bm{a}_z, \bm{t}\rangle$, which we detail below. While the
$\argmin$ in \eqref{eq:sparsemap} is generally not unique,
Carath\'eodory's theorem guarantees that solutions with support size
at most $D+1$ exist, and the active set algorithm enjoys linear and
finite convergence to a very sparse optimal distribution. Crucially,
\eqref{eq:sparsemap} has a solution $\bm{\xi}^\star$ such that the
set $\bar\ZZ = \left\{z \in \ZZ \mid \xi^\star_z > 0\right\}$ grows
only linearly with $D$, and therefore $|\bar\ZZ| \ll |\ZZ|$.

The active set method
\citep[Chapters 16.4 \& 16.5]{nocedalwright} as applied to the SparseMAP
optimization problem (Eq.~\ref{eq:sparsemap})~\cite{niculae2018sparsemap}
is a small variation of the formulation of Nocedal and Wright for handling
the equality constraint, due to ~\citet[Section 6]{ad3}.
Assume that we could identify the \emph{support}, or
\emph{active set} of an optimal solution  $\bm{\xi}^\star$, denoted
\[\bar\ZZ \defeq \left\{z \in \ZZ \mid \xi^\star_z > 0\right\}\,. \]
Then, given this set, we could find the solution to \eqref{eq:sparsemap}
by solving the lower-dimensional equality-constrained problem
%
\begin{equation}
    \minimize\| \bar{\bm{A}}\bar{\pv} - \bm{t} \|^2
    \quad\subjto\quad\bm{1}^\top\bar{\pv} = 1\,,
\end{equation}
%
where we denote by $\bar{\bm{A}}$ and $\bar{\pv}$ the restrictions of $\bm{A}$
and $\pv$ to the active set of structures $\bar\ZZ$.
The solution to this equality-constrained QP satisfies the KKT optimality
conditions,
%
\begin{equation}
    \label{eq:qp_kkt}
    \begin{bmatrix}
        {\bar{\bm{A}}}^\top \bar{\bm{A}} & \bm{1} \\
        \bm{1}^\top                      & 0      \\
    \end{bmatrix}
    \begin{bmatrix} \bar{\pv} \\ \tau \end{bmatrix}
    =
    \begin{bmatrix} \bar{\bm{A}}^\top \bm{t} \\ 1 \end{bmatrix}.
\end{equation}
%
Of course, the optimal support is not known ahead of time. The active set
algorithm attempts to guess the support greedily,
at each iteration either (if the solution of \eqref{eq:qp_kkt} is not feasible
for \eqref{eq:sparsemap})
dropping a structure from $\bar\ZZ$, or (otherwise) adding a new structure.
Since the support changes one structure at a time, the design matrix in
\eqref{eq:qp_kkt} gains or loses one row and column, so
we may efficiently maintain its Cholesky decomposition via rank-one updates.

We now give more details about the computation.
Denote the solution of Eq.~\ref{eq:qp_kkt}, (extended with zeroes),
by $\hat{\pv} \in \simplex^{|\ZZ|}$.
Since we might not have the optimal $\bar{\ZZ}$ yet, $\hat{\pv}$ can be infeasible
(some coordinates may be negative.)
To account for this, we take a partial step in its direction,
%
\begin{equation}
    \pv^{(i+1)} = (1-\gamma)\pv^{(i)} + \gamma \hat{\pv}^{(i+1)}\,
\end{equation}
%
where to ensure feasibility, the step size is given by
%
\begin{equation}\label{eq:step_size}
    \gamma = \min \left(1, \min_{z \in \bar{\ZZ}; \xi^{(i)}_z > \hat{\xi}_z}
    \frac{
        \xi^{(i)}_z
    }{
        \xi^{(i)}_z - \hat{\xi}_z}
    \right)\,.
\end{equation}

If, on the other hand, $\hat{\pv}$ is feasible for \eqref{eq:sparsemap},
(so $\gamma=1$),
we check whether we have a globally optimal solution.
By construction, $\hat{\pv}$ satisfies all KKT
conditions except perhaps dual feasibility $\bm{\nu} \geq 0$,
where $\nu_z$ is the dual variable (Lagrange multiplier) corresponding to the
constraint $\xi_z \geq 0$.
Denote $\bm{\mu}^{(i)} \defeq \bm{A}\pv^{(i)} = \bar{\bm{A}}\bar{\pv}^{(i)}$.
For any $z \not\in \bar{\ZZ}$, the corresponding dual variable must satisfy
%
\begin{equation}\label{eq:dual}
    \nu_z = \tau^{(i)} - \DP{\bm{a}_z}{\bm{t} - \bm{\mu}^{(i)}}\,.
\end{equation}
%
If the smallest dual variable is positive, then our current guess satisfies all
optimality conditions. To find the smallest dual variable we can equivalently
solve $\argmax_{z \in \ZZ} \DP{\bm{a}_z}{\bm{t} - \bm{\mu}^{(i)}}$, which is
a maximization (MAP) oracle call. If the resulting $\nu_z$ is negative,
then $z$ is the index of the most violated constraint $\xi_z \geq 0$;
it is thus a good choice of structure to add to the active set.

\begin{algorithm}[htpb]
    \caption{Active set algorithm for SparseMAP \label{alg:activeset}}
    \begin{algorithmic}[1]
        \Statex \textbf{Init:}
        $\bar\ZZ^{(0)} = \{ z^{(0)}\}$
        \quad where \quad
        $z^{(0)} \in \argmax_{z \in \ZZ} \DP{\bm{a}_z}{\bm{t}}$
        or a random structure.
        \For {i \text{in} $1, \ldots, N$}
        \State Compute $\tau^{(i)}$ and $\hat\pv^{(i)}$ by solving the relaxed QP (Eq.~\ref{eq:qp_kkt}).
        \Comment{Cholesky update.}
        \State $\pv^{(i)} \gets (1-\gamma) \pv^{(i-1)} + \gamma \hat{\pv}^{(i)}$
        ~(with $\gamma$ from Eq.~\ref{eq:step_size}).
        \If{$\gamma < 1$}
        \State Drop the minimizer of Eq.~\ref{eq:step_size} from $\bar{\ZZ}^{(i)}$.
        \Else
        \State Find most violated constraint,
        $z^{(i)} \gets \argmin_{z \in \ZZ} \nu_z$.
        \Comment{Eq.~\ref{eq:dual}, MAP oracle.}
        \If{$\nu_{z^{(i)}} \geq 0$}
        \State \Return \Comment{Converged.}
        \Else
        \State $\mathcal{Z}^{(i+1)} \gets \mathcal{Z}^{(i)} \cup \{ z^{(i)} \}$
        \EndIf
        \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}

The full procedure is given in Algorithm~\ref{alg:activeset}.
The backward pass is computed by implicit differentiation of \eqnref{eq:qp_kkt} \wrt
$\bm{t}$, giving, as in \cite{niculae2018sparsemap},
\begin{equation}
    \pfrac{\bar{\pv}}{\bm{t}} = \bar{\bm{A}}\big(\bm{S} - \bm{ss}^\top / s\big),
    \quad\text{where}\quad
    \bm{S} = (\bar{\bm{A}}^\top \bar{\bm{A}})^{-1},
    \bm{s} = \bm{S1}, s = \bm{1}^\top\bm{S1}
    \,.
\end{equation}
It is possible to apply the $\ell_2$ regularization term only to a subset of the
rows of $\bm{A}$, as is more standard in the graphical model literature. We
refer the reader to the presentation in \cite{ad3,niculae2018sparsemap} for this extension.
